## DONE

In this week (Week#18),

- For experiments: have studied the Rok's "devout-resonance-31" implementation. have implemented the siamese network. have run experiments with the tree-path model. have run experiments with the siamese model.
- For thesis draft: I mainly spent time on experiments recently so I did not do thesis writing.

## NEXT-STEPS

1. improve current implementations and make some checks
2. implement the CAT model, which performs very well with considering the syntactic info, or some similar ideas
3. run some experiments with the multi-modal model of (leaf-node + tree-path)
4. consider some improvements on siamese network (tree-leaf data, not the source-token sequence)

## QUESTIONS

None.

## Special Note

In this week, the biggest news is that Rok has published his "devout-resonance-31" implementation. I have prepared one extra VM to run his model and obtained very close results. Besides, I have checked his implementation and found he did not bring too much new things. There are two things not existing at the given NBoW baseline model. One is preprocessing the data, Rok removed stop-words and punctuations from natural language queries and removed punctuations from code snippets. The other one is adding some training tricks, like early-stopping. The former is to improve the accuracy and the latter is to reduce the training time. Other aspects, like padding, BPE, cosine loss are also at the given NBoW. I will continue the comparison.

Even though Rok seems not bring new things, but his implementation is really amazing. Rok is using Keras APIs and have upgraded TensorFlow to almost the latest version. Therefore, it is not very difficult to read his code and introduce new changes. Based on his implementation, I get able to implement the Siamese Network in just few days. Or else, I have to spend more days to complete the same work. I am not sure whether Siamese is better because I found there are some issues at the implementation of generating predictions locally for computing NDCG scores, so I have no data for any other NDCG scores. However, it seems not very good according to the MRR score, the best MRR score is small than 0.01 (because maybe there are some issues in computing MRR scores, I plan to make some checks in the next week.) Actually, the given baseline models are using pseudo-Siamese architecture. It seems more ideal than Siamese because for each pair of code snippet and query, the former usually have more tokens than the latter (max number of tokens: 200 v.s. 30). My idea is to use tree-leaf data to replace source tokens, so that both code and query will have similar length.

For the multi-modal model, I have tested over the tree-path data. I have not tested the multi-modal model but its implementation is ready. I plan to run its experiments in the next week. The existing issue is that it would take much more time on training and evaluation (1.848 hours v.s. 0.365 hours for tree-path comparing with NBoW). Besides, I found CAT model is another good model for introducing syntactic info into the semantic code search task. CAT model is very promising because it has been tested on the semantic code search. I will consider some alternative plans to the tree-path representation, just as listed at the background section of the <A Multi-Perspective Architecture for Semantic Code Search> paper.

<!-- I want to check its accuracy over the Python corpus in next days because Considering that my best model can make its NDCG score be around 0.312 for the Python corpus, maybe it can also have better NDCG score than Rok's model over the whole corpus. I have not yet tested my model over the whole corpus but I am going to test it at the next week. If it performs close to Rok's model, then we can see that the best models in the challenge are just NBoW + preprocessing. (By the way, there is no special with the name "devout-resonance-31", it is randomly generated by W&B, others are like "smooth-smoke-233".) -->
